{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the mean absolute error:\n",
    "$$\n",
    "MAE = \\frac{1}{N}\\sum_{i=1}^N |y_i-x_i^\\top\\theta|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MAE(theta, X, y):\n",
    "    mae = np.average(np.abs(y - X@theta.T), axis=0)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7.2\n",
    "Implement the problem from Question 7.1 (Use the `GLPK` solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X, X_test, Y, Y_test = train_test_split(diabetes['data'], \n",
    "                                        np.expand_dims(diabetes['target'], 1), \n",
    "                                        test_size=0.25, random_state=0)\n",
    "X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1) # Bias column:\n",
    "X_test = np.concatenate((X_test, np.ones((X_test.shape[0], 1))), axis=1)\n",
    "\n",
    "train, validation, thetas, optimal = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter:\n",
    "lambda_ = np.logspace(-5, -1, 50, base = 10)\n",
    "\n",
    "#LP solver:\n",
    "def LP_solver(X, Y, lambda_, k):\n",
    "    d = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "    Beta = cp.Variable((N, 1))\n",
    "    b = cp.Variable((d, 1))\n",
    "    alpha = cp.Variable((1,1))\n",
    "    theta = cp.Variable((1, d))\n",
    "    LP = cp.Problem( cp.Minimize(k * alpha + cp.sum(Beta) + lambda_ * cp.sum(b)),               [\n",
    "        alpha + Beta >= 1/k * (Y - X @ theta.T),\n",
    "        alpha + Beta >= -1/k * (Y - X @ theta.T),\n",
    "        Beta >= 0,\n",
    "        -b <= theta,\n",
    "        theta <= b\n",
    "    ])\n",
    "    LP.solve()\n",
    "    optimal_theta = theta.value\n",
    "    optimal_value = LP.value\n",
    "    dual_value = LP.constraints[0].dual_value\n",
    "    return optimal_theta,  optimal_value, dual_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Optimal values--\n",
      "Optimal lambda: 0.004094915062380423\n",
      "Optimal value: 69.85032419743328\n",
      "Optimal theta:\n",
      " [[ -51.25036457 -256.59297979  256.59298077  256.59298071  134.15685779\n",
      "  -256.59297983 -256.59298057  256.59298031  256.59298072  249.77586678\n",
      "   151.12806908]]\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation:\n",
    "for l in lambda_:\n",
    "    optimal_theta, optimal_value, dual_value = LP_solver(X, Y, l, math.floor(0.75 * X.shape[0]))\n",
    "    thetas.append(optimal_theta)\n",
    "    optimal.append(optimal_value)\n",
    "    train.append(get_MAE(optimal_theta, X, Y))\n",
    "    validation.append(get_MAE(optimal_theta, X_test, Y_test))\n",
    "    \n",
    "best_lambda = lambda_[np.argmin(validation)]\n",
    "best_theta = thetas[np.argmin(validation)]\n",
    "best_value = optimal[np.argmin(validation)]\n",
    "\n",
    "print('---Optimal values--')\n",
    "print(f'Optimal lambda: {best_lambda}')\n",
    "print(f'Optimal value: {best_value}')\n",
    "print(f'Optimal theta:\\n {best_theta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of training : [46.42683702]\n"
     ]
    }
   ],
   "source": [
    "print('MAE of training : {}'.format(get_MAE(best_theta, X, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of test : [43.17935494]\n"
     ]
    }
   ],
   "source": [
    "print('MAE of test : {}'.format(get_MAE(best_theta, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant difference between the MEA of the training set and the MEA of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
